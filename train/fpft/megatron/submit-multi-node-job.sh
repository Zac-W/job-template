#!/bin/bash

export NNODES=${NNODES:-2}
export WORKER_NODES=$((NNODES - 1))
export MODEL_NAME=${MODEL_NAME:-Meta-Llama-3.1-8B-Instruct}
export GPUS_PER_NODE=${GPUS_PER_NODE:-8}
# export EPOCHS=${EPOCHS:-2}
# export TRAIN_BATCH_SIZE_PER_DEVICE=${TRAIN_BATCH_SIZE_PER_DEVICE:-2}
# export DATASET_TEMPLATE=${DATASET_TEMPLATE:-llama3}
# export REPORT_TO=${REPORT_TO:-tensorboard}
# export LOGGING_DIR=${LOGGING_DIR:-/app/output/logs}
export IMAGE_NAME=${IMAGE_NAME:-10.5.1.249/ebtech-website/megatron-lm:v0.8.0rc0-torch24.01-py3-20241105-105705}
export MICRO_BATCH_SIZE=${MICRO_BATCH_SIZE:-1}
export GLOBAL_BATCH_SIZE=${GLOBAL_BATCH_SIZE:-16}
export SAVE_INTERVAL=${SAVE_INTERVAL:-50}
export TRAIN_ITERS=${TRAIN_ITERS:-60000}
export EVAL_ITERS=${EVAL_ITERS:-200}
export NUM_LAYERS=${NUM_LAYERS:-32}
export HIDDEN_SIZE=${HIDDEN_SIZE:-4096}
export FFN_HIDDEN_SIZE=${FFN_HIDDEN_SIZE:-14336}
export NUM_ATTENTION_HEADS=${NUM_ATTENTION_HEADS:-32}
export SEQ_LENGTH=${SEQ_LENGTH:-8192}
export MAX_POSITION_EMBEDDINGS=${MAX_POSITION_EMBEDDINGS:-131072}
export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:-bond0}
export DATA_PATH=${DATA_PATH:-/dataset/books-converted/my-book-data_text_sentence}
export MEGATRON_LOGGING_LEVEL=${MEGATRON_LOGGING_LEVEL:-1}
export TP=${TP:-2}
export PP=${PP:-4}
export LAYER_PER_VPP=${LAYER_PER_VPP:-4}

envsubst '$MODEL_NAME $NNODES $WORKER_NODES $GPUS_PER_NODE \
          $IMAGE_NAME \
          $MICRO_BATCH_SIZE $GLOBAL_BATCH_SIZE $SAVE_INTERVAL \
          $TRAIN_ITERS $EVAL_ITERS $NUM_LAYERS $HIDDEN_SIZE \
          $FFN_HIDDEN_SIZE $NUM_ATTENTION_HEADS $SEQ_LENGTH \
          $MAX_POSITION_EMBEDDINGS $NCCL_SOCKET_IFNAME $DATA_PATH \
          $MEGATRON_LOGGING_LEVEL $TP $PP $LAYER_PER_VPP \
        ' < example-pytorchjob.tpl.yaml | kubectl replace --force -f -

